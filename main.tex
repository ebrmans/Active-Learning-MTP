\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern} % for tilde 
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage{booktabs}
\usepackage{minted}
\usepackage{float}
\usepackage{titlesec} % for customizing chapter/section spacing
\usepackage{tabularx}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algpseudocode}

\title{Numerical study on small cell training protocol for Pb system}
\author{YM Chan}
\date{}

\begin{document}
% generate title on new page
\maketitle

\section{Introduction}

This report summarizes work conducted in Fall 2025 on the small cell training protocol, an active learning approach for developing machine-learned (ML) interatomic potential models for large atomic systems proposed by Meng et al. \cite{small-cell}. The objective is to train models that approximate density functional theory calculations by predicting atomic forces and system energies using reference DFT data.

The small cell training protocol addresses the high computational cost of generating DFT data for large systems. Training begins with small atomic cells and progressively increases system size until the target configuration is reached. This approach is effective because atomic behavior in large systems is largely governed by local interactions, allowing models trained on small cells to generalize to larger structures.

In this study, the small cell training protocol was implemented for a lead-based system. The following milestones were achieved:

\begin{enumerate}
  \item Optimization of MLIP-3 hyperparameters for lead
  \item Development of Python scripts to automate active learning between LAMMPS, MLIP-3, and VASP
  \item Modification of the MLIP-3 source code to track the extrapolation factor $\gamma$
  \item Execution of active learning on the [1,1,1] lead unit cell
  \item Evaluation of model performance across iterations
  \item Initial attempts to scale to the [1,1,2] system on the Pitagora HPC platform
\end{enumerate}

\begin{table}[H]
    \caption{Nomenclature/ abbreviations}
    \begin{center}
    \begin{tabularx}{\linewidth}{|p{0.25\linewidth}|X|}
      \toprule
      Name  & Description  \\
      \midrule
      LAMMPS & Open-source Molecular Dynamic simulation software \cite{lammps} \url{https://www.lammps.org/#gsc.tab=0} \\
      VASP   & Ab-initio Density Functional Theory simulation software \cite{vasp} \url{https://www.vasp.at/} \\
      MLIP-3 & Open-source Machine learning model for interatomic potentials \cite{mlip} \url{https://gitlab.com/ashapeev/mlip-3} \\
      ML     & Machine learning \\
      \bottomrule
    \end{tabularx}
    \label{tab:nomenclature}
  \end{center}
\end{table}


Suggestions for future work:
\begin{enumerate}
  \item{Completing the implementation of small cell training protocol on HPC Pitagora and train on larger systems.}
  \item{Perform more rigourous numerical study of error convergence at larger unit cells}
  \item{Convergence and comparison study of MLIP-3 with other ML models, e.g. Deepmd.}
\end{enumerate}

\section{MLIP-3 model hyperparameter selection}

MLIP-3 is a ML method developed by Podryabinkin et al. based on a basis function set comprising of different orders of moment tensor products of the atom positions. The main model hyperparameter is the ``MTP level'', a measure of the number of basis functions to be included in the model. The higher the number the more higher order moments are included in the basis function set. 

\subsection{Training data description}

In order to choose the optimal MTP level for the Pb system, I trained different MTP levels on the same training data set generated by Qiu Guo. The training data set consist of 4 different simulations, comprising of [2,2,2] and [3,3,3] unit cells, Pb unit cell = 4 atoms, so 32 and 108 atoms respectively. Once trained, the models are tested on 2 previously unseen datasets (1) Pb atoms in equilibrium configuration and (2) Pb atoms in non-equilibrium configurations. 

Breakdown of training data
\begin{table}[H]
  \caption{Breakdown of training data distribution}
  \begin{center}
    \begin{tabular}{cc}
      \toprule
      Name  & \# configs  \\
      \midrule
       eq-32  & 1020  \\
       eq-108 & 1020 \\
       neq-32 & 1980 \\
       neq-108 & 1980 \\
      \bottomrule
    \end{tabular}
    \label{tab:training_data_distribution}
  \end{center}
\end{table}

\paragraph{Results of MTP level versus model accuracy.}
Model accuracy is evaluated using a set of test configurations that are not included in the training data. For each test configuration, reference atomic forces
\[
F_{\text{ref}} = [F_x, F_y, F_z]
\]
and total energy \(E_{\text{ref}}\) are computed using VASP. The trained MLIP-3 model is then used to predict the corresponding forces \(F_{\text{ML}}\) and energy \(E_{\text{ML}}\). Model error is quantified by comparing the predicted values against the reference results. We define the error of the force vector as the root mean square (RMS) over all coordinates for every atom in the lattice:

\begin{equation} \label{eqn:error_rmse_f}
  \epsilon_{F} = \sqrt(\frac{1}{I} \sum_{i} ||F_{ref,i} - F_{ML,i}||_{2})
\end{equation}

and the error of the lattice energy is simply the absolute error:

\begin{equation} \label{eqn:error_e}
\epsilon_{E} = |E_{ref} - E_{ML}|
\end{equation}

Across a set of lattices, we take the average of error metrics 

\begin{equation}
  \bar{\epsilon}_{F} = \text{Mean}( \epsilon^{1}_{F}, \epsilon^{2}_{F} ,...)
\end{equation}

\begin{equation}
  \bar{\epsilon}_{E} = \text{Mean}( \epsilon^{1}_{E}, \epsilon^{2}_{E} ,...)
\end{equation}


We decided to select MTP level=18 which shows stagnating accuracy. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/energy_err_convergence.pdf}    
  \caption{Convergence of RMSE energy with MTP level.}  
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/energy_err_convergence.pdf}    
  \caption{Convergence of RMSE forces with MTP level.}  
\end{figure}

Summary of training times

\begin{table}[H]
    \caption{CPU-training times for various MTP levels.}
    \begin{center}
      \begin{tabular}{cc}
        \toprule
        MTP level & CPU-time  \\
        \midrule
         6 & 56 mins  \\
         12 & 6.7 hours \\
         18 & 47 hours \\
         24 & 496 hours \\
         28 & $>48$ days [killed] \\
        \bottomrule
      \end{tabular}
      \label{tab:training_times}
    \end{center}
\end{table}

\section{Small cell training implementation}

\subsection{Description of MLIP-3/LAMMPS interface}

A newer version, MLIP-4, was available but caused errors with the LAMMPS interface, so we continued using MLIP-3. MLIP-3 integrates with LAMMPS, and the input line specifies the trained model location along with additional parameters. (\verb|threshold_save|, \verb|threshold_break|,...): 

\begin{minted}[breaklines]{python}
  pair_style mlip      load_from=<pot.almtp> extrapolation_control=true threshold_save=0.1 threshold_break=10  extrapolation_control:save_extrapolative_to=<preselected.cfg>
\end{minted}

Description of MLIP-3 interface with LAMMPS
\begin{table}[H]
    \caption{Interface options between MLIP-3 and LAMMPS}
    \begin{center}
    \begin{tabularx}{\linewidth}{|p{0.35\linewidth}|X|}
      \toprule
      Name  & Description  \\
      \midrule
      \verb|load_from:<file>| & Path to trained MLIP-3 model \\
      \verb|extrapolation_control| & Boolean. See below for logic control.\\
      \verb|threshold_save|  & Value of $\gamma_{save}$. See below. \\
      \verb|threshold_break|  & Value of $\gamma_{break}$. See below. \\
      \verb|save_extrapolative_to|  & Path to save atomic configurations from LAMMPS with $\gamma > \gamma_{save}$ as a text file.\\
      \bottomrule
    \end{tabularx}
    \label{tab:nomenclature_pairstyle_mlip}
  \end{center}
\end{table}

The MLIP-3 authors introduce an extrapolation factor $\gamma$, which is a scalar value quantifying how different a queried atomic configuration to the model is from the set of training data during model training. The value of $\gamma = 1$ is obtained when a queried atomic configuration matches exactly one of the training configurations. The authors suggest a value of $2 < \gamma < 10$ as ``safe-extrapolation'' and $\gamma > 10$ as ``unsafe-extrapolation''. 

During a coupled LAMMPS-MLIP3 simulation, MLIP-3 will monitor the value of the extrapolation factor based on queried atomic configurations ($\gamma_{q}$). If $\gamma_{q}$ exceeds the user-set parameters [$\gamma_{save}$, $\gamma_{break}$], MLIP-3 will perform the corresponding action: 
\begin{itemize}
  \item{$\gamma_{q} > \gamma_{save}$: MLIP-3 continues LAMMPS-MLIP3 simulation, writing out the configuration to \verb|preselected.cfg|.}
  \item{$\gamma_{q} > \gamma_{break}$: MLIP-3 ends LAMMPS-MLIP3 simulation after writing out the configuration to \verb|preselected.cfg|.}
\end{itemize}

\begin{table}[H]
  \caption{Podryabinkin et al. suggested values of [$\gamma_{save}$, $\gamma_{break}$] \cite{mlip} }
  \begin{center}
    \begin{tabular}{cc}
      \toprule
      Parameter & Value \\
      \midrule
      $ \gamma_{save} $      & 2.1 \\
      $ \gamma_{break} $ & 10.0 \\
      \bottomrule
    \end{tabular}
    \label{tab:values_of_gamma}
  \end{center}
\end{table}

The small cell training protocol builds upon MLIP-3 extrapolation logic control to create new datasets using ab-initio code to simulate the configurations in \verb|preselected.cfg|. We define the set of extrapolated configurations in \verb|preselected.cfg| as $\mathcal{D}_{save}$ and the set of configurations to train the model as $\mathcal{D}$. For a given lattice configuration (e.g [1,1,2] unit cell) the active learning protocol is as follows

\begin{algorithm} [H]
  \caption{Active learning to reduce extrapolation grade}
  \label{alg:active_learning}
  \begin{algorithmic}[1]
  \Procedure{Active learning}{$L, \mathcal{D}$}
      \State Train model on $\mathcal{D}$
      \While {$\max{\gamma} > \gamma_{break}$}
        \State \Call{Write LAMMPS input}{L}
        \State LAMMPS/MLIP-3 simulation(s) with extrapolation logic control to obtain $\mathcal{D}_{save}$
        \For{$C_{i} \in \mathcal{D}_{save}$}
          \State Run VASP simulation on $C_{i}$
        \EndFor
        \State Append new training configurations to dataset $\mathcal{D} \gets \mathcal{D} \cup \mathcal{D}_{save}$
        \State Retrain model on $\mathcal{D}$
      \EndWhile 
      \State \Return $\mathcal{D}$
  \EndProcedure
  \end{algorithmic}
\end{algorithm}

The small cell training protocol applies active learning on progressively larger lattices. Let 
\(\mathcal{L} = \{ L_1, L_2, \dots \}\) with \(L_1\) smaller than \(L_2\). For practical simulations using LAMMPS/MLIP-3, temperature and pressure ranges 
\((\mathcal{T}, \mathcal{P})\) must also be specified. Following Meng et al., temperatures and pressures are sampled from a grid, and atomic positions are slightly perturbed from their lattice sites with length parameter \(a\). The procedure is defined as:


\begin{algorithm} [H]
  \caption{Write LAMMPS input}
  \label{alg:write_lammps_input}
  \begin{algorithmic}[1]
  \Procedure{Write LAMMPS input}{$L, (\mathcal{T}, \mathcal{P}), a$}
      \For{ $(T, P) \in (\mathcal{T}, \mathcal{P}) $}
        \For{atom $i$ in lattice}
           \State Calculate ``default'' atom position in lattice $x_{i}, y_{i}, z_{i} \gets f(L, a)$
           \State Generate random perturbations in coordinates ($\delta_{x}, \delta_{y}, \delta_{z} $)
           \State $x_{i}, y_{i}, z_{i} \gets (x_{i}, y_{i}, z_{i}) + (\delta_{x}, \delta_{y}, \delta_{z})$
        \EndFor
        \State Write out LAMMPS file with atomic positions, $T$ and $P$
      \EndFor
  \EndProcedure
  \end{algorithmic}
\end{algorithm}

The small cell protocol algorithm is therefore: 

\begin{algorithm} [H]
\caption{Small cell training protocol}
\label{alg:small_cell_training_protocol}
\begin{algorithmic}[1]
\Procedure{Small Cell protocol}{}
  \State Dataset with initial training conditions $\mathcal{D}$
  \For{$L_{i} \in \mathcal{L} $}
    % \State Perform active learning step (Alg. ) 
    \State \Call{Active Learning}{$L_{i}, \mathcal{D}$}
    \State $\mathcal{D} \gets \mathcal{D} \cup \mathcal{D}_{save}$
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Questions on active learning mechanism}
To better understand the process of the coupled active learning mechanism, we decided to focus on the performance of Algorithm (\ref{alg:active_learning}), answering several key questions 

\begin{enumerate}
  \item{$\gamma$ is used to determine if a simulation is ``good''. How does it actually correlate with error (i.e. the difference between ML model and VASP calculation)?}
  \item{For a run of the active learning algorithm (Alg. \ref{alg:active_learning}) at a set lattice configuration at constant temperature and pressure, how many iterations i.e. step (3-11) are needed?}
  \item{As $\mathcal{D}$ grows, two things are expected: (1) Max and average $\gamma$ should decrease (2) actual error should decrease. Do we observe this decrease and how fast do they decrease?}
\end{enumerate}

\subsection{Answers to active learning questions}

\subsubsection{$\gamma$ is used to determine if a simulation is ``good''. How does it actually correlate with error (i.e. the difference between ML model and VASP calculation)?}

% note: original file for this exercise is stored at: /home/yimeng/pb-dpmodel/mlip3_ym_test/small_cell_training/test_err_vs_extrapolation_grade 
The trained MLIP-3 model (MTP level = 18) was tested using a LAMMPS/MLIP-3 simulation. Configurations were generated with a wide range of $\gamma$ values by setting a lower saving threshold, $\gamma_{\rm save} = 0.1$, and configurations with $\gamma$ uniformly distributed between 0.1 and 10 were then selected. In practice, however, the majority ($>99\%$) of $\gamma$ values are concentrated between 0.1 and 2.1, as shown by the distributions in the later sections.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/exgrade_rmseforce.pdf}    
  \caption{Relationship between $\epsilon_{F}$ (see Eqn \ref{eqn:error_rmse_f}) and $\gamma$}  
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/exgrade_ene.pdf}    
  \caption{Relationship between $\epsilon_{E}$ (see Eqn \ref{eqn:error_e}) and $\gamma$}  
\end{figure}

The correlation of $\epsilon_F$ is much stronger than that of $\epsilon_E$. This may be because the energy reflects the entire lattice, so small random perturbations in atomic configurations tend to produce errors that partially cancel. In contrast, force errors are summed over all atoms, making them more sensitive to differences between training and testing configurations.


Conclusion: $\gamma$ is strongly correlated with predicted force error, so reducing $\gamma$ will most likely lead to lower predicted force error. 

\subsection{For a run of the active learning algorithm (Alg. \ref{alg:active_learning}) at a set lattice configuration at constant temperature and pressure, how many iterations i.e. step (3-11) are needed?}

A python script was created to run the active learning algorithm (Alg \ref{alg:active_learning}). A [1,1,1] unit cell was used with set temperature and pressure at 300 K and 1 bar using the ``fix npt/body command'' in LAMMPS. The active learning iterations were carried out for 60 iterations with $\gamma_{break}=10.0$. MLIP-3 $\gamma_{save}=0.1$ was used to monitor the distributions of $\gamma$ and to save almost all configuration queries between LAMMPS and MLIP-3, but only configurartions with $\gamma > 2.1$ were sent for VASP calculations to emulate $\gamma_{save}=2.1$. 

The figure below shows the number of VASP calculations per active learning iteration. The total number of VASP calculations over 60 iterations is 20308.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/number_of_vasp_runs_over_iterations.pdf}    
  \caption{Number of VASP calculations against active learning iteration}  
\end{figure}

Notice that there is a wide variation in the number of VASP calculations per iteration, ranging from 100 to 1400 VASP calculations. This variation makes it more difficult to implement this system in HPC framework (see below for recommendations).At iteration 52, maximum $\gamma$ values are less than $\gamma_{break}$ so the LAMMPS simulation is not prematurely terminated by MLIP-3. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/max_extrapolation_grade_over_iterations.pdf}    
  \caption{Maximum extrapolation grade against active learning iteration with $\gamma_{break}=10.0$}  
\end{figure}

\subsection{As $\mathcal{D}$ grows, two things are expected: (1) Max and average $\gamma$ should decrease (2) actual error should decrease. Do we observe this decrease and how fast do they decrease?}

\subsubsection{(1) Max and average $\gamma$ should decrease}

Max $\gamma$ follows an unpredictable trend as shown in the previous section. In the following figure we show the mean and standard deviation of $\gamma$ over active learning iterations, which show the mean decreasing from around 1.0 to around 0.5. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/mean_stddev_extrapolation_grade_over_iterations.pdf}    
  \caption{Mean and standard deviation of extrapolation grade against active learning iteration}  
\end{figure}

The following figures show the histogram distribution of $\gamma$ for queries between LAMMPS and MLIP-3 during a coupled simulation at an early iteration (iter = 1), mid-point (iter = 30), and final iteration (iter = 55). In all cases, over 99.9\% of $\gamma$ values are below 2.0, indicating that the model operates mostly within the interpolation or ``safe-extrapolation'' regime relative to the training data. At iterations 1 and 30, MLIP-3 prematurely terminates the LAMMPS simulations, resulting in \verb|~|20000 and \verb|~|50000 calls to MLIP-3, respectively. By iteration 55, with no premature termination, the number of calls increases to \verb|~|100000.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/hist_mv.pdf}    
  \caption{Distribution of $\gamma$ at iteration 1}  
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/hist_mv.pdf}    
  \caption{Distribution of $\gamma$ at iteration 30}  
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/hist_mv.pdf}    
  \caption{Distribution of $\gamma$ at iteration 55}  
\end{figure}

Conclusion: Mean $\gamma$ decreases slowly over active learning iterations but with standard deviation of $\gamma$ for each active learning iteration being much larger the overall trend. The maximum $\gamma$ does not show any trend, but rather drops to small values very suddenly. The reason for this is unknown, possibly due to how LAMMPS simulation is carried out and its interaction with MLIP-3. 

\subsubsection{(2) actual error should decrease}

In typical ML convergence studies, the mean prediction error decreases as the number of training points increases independently. In dynamic active learning, this is less straightforward because only configurations with ``high'' extrapolation factors are added to the training set, providing feedback from model queries. This selective sampling is expected to improve convergence. To examine this effect, 200 random configurations were drawn from all queries between LAMMPS and MLIP-3. These configurations were evaluated with VASP and compared to MLIP-3 predictions of atomic forces and energies, with errors $\epsilon_F$ and $\epsilon_E$ computed as defined in Equations~(\ref{eqn:error_rmse_f}) and (\ref{eqn:error_e}), respectively.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/energy_error_over_iterations.pdf}    
  \caption{$\epsilon_{E}$ over iterations}  
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.96\columnwidth]{images/forces_error_over_iterations.pdf}    
  \caption{$\epsilon_{F}$ over iterations}  
\end{figure}

The energy convergence plot behaves as expected, with errors decreasing over successive iterations. In contrast, the force error shows an unexpected increase despite the growing dataset, which should provide more information for learning. This anomaly is unlikely due to incorrect active learning implementation, as $\epsilon_E$ decreases, but may arise from improper VASP inputs or a potential bug in MLIP-3. Future work is suggested to reproduce and investigate this behavior. In summary, energy prediction errors decrease with active learning iterations, while force prediction errors surprisingly increase.

\section{Summary and suggestions for future work}

In this work we performed numerical investigation of the active learning step using MLIP-3's extrapolation control logic as one of the key components of the small cell training protocol. We tested on a lead based system with the [1,1,1] unit cell. 

\begin{itemize}
  \item{Sensitivity study showed that MTP level=18 is good for lead.}
  \item{Total number of VASP calculations was \verb|~|20000.}
  \item{Max. $\gamma$ decreases to $<2$, bringing it within the range of `safe extrapolation'.}
  \item{Energy error decreases but strangely, force error seems to increase.}
\end{itemize}

Suggestions for future work
\begin{enumerate}
  \item{Completing the implementation of small cell training protocol on HPC Pitagora and train on larger systems.}
  \item{Perform more rigourous numerical study of error convergence at larger unit cells}
  \item{Convergence and comparison study of MLIP-3 with other ML models, e.g. Deepmd.}
\end{enumerate}

\section{Code development and implementation}

In this work, I wrote python files to script execution of algorithms \ref{alg:active_learning} - \ref{alg:small_cell_training_protocol}, which are based on the python files of the small cell training group located at \url{https://github.com/RichardZJM/Small-Cell-MTP-Training} with significant changes. This section documents the codes created, how they should be used and where they are found in the folder ``codes''. A public github repo has been created for this purpose, found here: \url{https://github.com/YimengChankth/active-learning-ML-potential-KTH.git}

\subsection{codes/active-learning}

The main python file to handle the sequential code executions is: \verb|codes/active_learning/main.py| which performs the following code execution (method: \verb|lammps_phase|) for an active learning iteration
\begin{enumerate}
  \item{Create a new directory: \verb|iter/<i>|}
  \item{\verb|generate_lamnps_file|: Write out new LAMMPS input file}
  \item{Run LAMMPS}
  \item{\verb|sample_query_error|: Choose 200 random configurations with $0.1<\gamma<10$ and run VASP simulations (folder: \verb|iter/<i>/vasp|)  to perform iteration error analysis.}
  \item{\verb|run_new_batch_cfg_with_threshold|: Select all configurations with $2.1<\gamma<10$ and run VASP simulations (folder: \verb|iter/<i>/vasp_threshold|) to generate new training data}
  \item{\verb|train_new_MLIP_model|: Append new training data and train new MLIP-3 model, saving the model parameters (file: \verb|iter/<i>/updated_model/pot.almtp|)}
\end{enumerate}

\subsection{codes/Small-Cell-MTP-Training}

This is a forked version of the Small cell MTP training from \url{https://github.com/RichardZJM/Small-Cell-MTP-Training}. The main difference was to include a way to write FCC configuration, as the original only had BCC configurations specifically in the file \verb|codes/Small-Cell-MTP-Training/SmallCellMTPTraining/io/writer.py|. 

\begin{minted}[breaklines]{python}
  # look for the following line

  def writeMDInput_fcc(fileName: str, jobProperties: dict):
    """Creates a MD Input for fcc e.g. Pb, needing:
    mdProperties = ["latticeParameter", "boxDimensions", "potFile", "temperature", "pressure","elements", "atomicWeights"]
    """
    properties = props.mdProperties
    if checkProperties(properties, jobProperties):
        raise Exception(
            "Properties missing: " + checkProperties(properties, jobProperties)
        )
    num_elem = len(jobProperties["elements"])
    # Calculate the number of unit cells in each dimension
    nx = int(np.ceil(jobProperties["boxDimensions"][0]))
    ny = int(np.ceil(jobProperties["boxDimensions"][1]))
    nz = int(np.ceil(jobProperties["boxDimensions"][2]))
    # Total number of atoms
    num_atoms = 4 * nx * ny * nz  # 4 atoms per unit cell in FCC
  ...
\end{minted}

MLIP-3 and MLIP-3 LAMMPS interface are used ``as-is''. 

\begin{enumerate}
  \item{MLIP-3: \url{https://gitlab.com/ashapeev/mlip-3}}
  \item{MLIP-3 LAMMPS interface \url{https://gitlab.com/ivannovikov/interface-lammps-mlip-3/}}
\end{enumerate}

\subsection{codes/MTPlevel\_sensitivity}

This folder contains the python script used to test different MTP levels. See \verb|main.py|. 

The specific portion is 
\begin{minted}[breaklines]{python}
  def train_mlip_model():
    # initial test for MLIP model and varying the levels 

    MTP_levels = [8, 10, 14, 16, 20, 22, 28]

    # train phase 
    for mtp_count, mtp_value in enumerate(MTP_levels):
        mlip_model = mlipModel(savedir=f'mlip_ym_all_train/mtp_{mtp_value}')
        outcar_files, _ = get_all_training_outcars()  
        mlip_model.training_outcars = outcar_files
        mlip_model.MTP_level = mtp_value
        shutil.copy(f'mlip_ym_all_train/mtp_6/train.cfg', mlip_model.savedir)
        mlip_model.train(mpi=16)

    pass

\end{minted}

\subsection{Access to training data}

As github is not meant to store large amounts of training data, this section points to where the training data may be found in Southpole. 

\subsubsection{MTP level sensitivity training and testing data}

The training and testing data used for the MTP level sensitivity study contains equilibrium and non-equilibrium data sets for 32 and 108 atoms are currently with Qiu Guo. On Southpole the last updated location is at: 

\begin{verbatim}
/home/qiuguo/pb-dpmodel/OUTCARS/
|-- 108eqOUTCARs/<outcar files>
|-- 108neqOUTCARs/<outcar files>
|-- 32eqOUTCARs/<outcar files>
|-- 32neqOUTCARs/<outcar files>
|-- pb_testdata/
|   |-- 108/
|   |   |-- eq/<outcar files>
|   |   `-- neq/<outcar files>
|   `-- 32/
|       |-- eq/<outcar files>
|       `-- neq/<outcar files>
\end{verbatim}

The OUTCARS used for training are: [108eqOUTCARs, 108neqOUTCARs, 32eqOUTCARs, 32neqOUTCARs] and \verb|pb_testdata| contains the testing data. 

\subsubsection{Active learning iteration data}

At each active learning iteration from $i = \{0,1,...60\}$, the training data set is saved and stored in: 

\begin{minted}[breaklines]{text}
  /home/qiuguo/pb-dpmodel/mlip3_ym_test/small_cell_training/\
    test_iterative_training/iter/<i>/updated_model/train.cfg
\end{minted}

And the model trained on the updated model is located in the same folder:
\begin{verbatim}
  /home/qiuguo/pb-dpmodel/mlip3_ym_test/small_cell_training/\
    test_iterative_training/iter/<i>/updated_model/pot.almtp
\end{verbatim}

The file size of train.cfg increases due to the continuous appending of new training configurations to $\mathcal{D}$. At iteration 1, the train.cfg file is 44MB and at iteration 60, the train.cfg file is 57MB. 
Iteration 1, i.e. the initial training data set consists of the 32 and 108 configurations used in the MTP level sensitivity study. 

\subsection{Difficulties of porting to HPC systems}
I need to stress that the code development was on a private computer of the group (southpole) and not on supercomputing resources. This environment allowed complete freedom to run simulations continuously without concerns about job time limits, unexpected shutdowns, or queue-related cancellations. As a result, the scripting and workflow were designed around unrestricted, long-running execution.

Initial efforts to adapt the code for HPC systems were started on Pitagora but could not be completed. Porting the workflow to HPC environments is expected to be challenging, particularly due to incompatibilities with standard SLURM-based job submission practices. Typical HPC usage favors a small number of large jobs with predefined runtimes, often less than one day. In contrast, the current workflow relies on executing hundreds of small VASP calculations at reduced lattice sizes, which conflicts with common HPC scheduling policies.

Specifically, deploying the algorithm on HPC systems must address the following challenges:

\begin{enumerate}
  \item The active learning process is inherently dynamic, and the total number of required VASP calculations is not known in advance.
  \item Each active learning iteration involves hundreds of VASP calculations at smaller lattice sizes, potentially exceeding limits on the number of simultaneously submitted jobs.
  \item After all VASP calculations in an iteration are completed, the machine learning model must be retrained. Triggering this step is nontrivial because the number of submitted VASP jobs is not predetermined.
  \item A similar dependency exists for the LAMMPS simulations, which must be triggered only after model retraining has completed.
\end{enumerate}

A potential solution is to reserve a large allocation of computing resources for a fixed time window and manage the entire workflow using a Python script, rather than relying on individual SLURM job submissions. This script would handle VASP calculations, machine learning interatomic potential training, and LAMMPS simulations internally, while monitoring progress and recording completed tasks in a persistent checklist. If the reservation expires, the checklist can be used to resume the workflow from the previous state. Under this approach, the user would only need to submit a single job request periodically, for example every 24 hours, while the Python script manages all intermediate steps.



% Table example 
% Table (\ref{tab:all_variable_descriptions})

% \begin{table}[H]
%   \caption{List of input and target variables}
%   \begin{center}
%     \begin{tabular}{cl}
%       \toprule
%       \multicolumn{2}{c}{Input variables ($x$)} \\
%       \midrule
%       Parameter & Description \\
%       \midrule
%       $ x_{XE135} $ & Xe-135 concentration \\
%       $ x_{SM140} $ & Sm-140 concentration \\
%       $ y_{D1} $ & E-group 1 diffusion coefficient \\
%       $y_{D2}$ & E-group 2 diffusion coefficient\\
%       \bottomrule
%     \end{tabular}
%     \label{tab:all_variable_descriptions}
%   \end{center}
% \end{table}

% Figure example 
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\textwidth]{images/image0.png} % replace with your image
%     \caption{Example figure.}
%     \label{fig:example}
% \end{figure}

% note minted usage with breaklines
% minted environment
% \begin{minted}[breaklines]{python}
%     if __name__ == "__main__":
%         pass
% \end{minted}
% To see full list of language supported: \verb|pygmentize -L lexers|

\bibliographystyle{unsrt}
\bibliography{references}




\end{document}